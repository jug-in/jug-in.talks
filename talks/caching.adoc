:title: Caching
//:author: Bernd St√ºbinger
//:duration: 45min
include::_theme/theme.adoc[]
:imagesdir: talks/caching
:src: caching

_How to not shoot yourself in the foot_

[%notitle,role="center"]
== Quote

[,"Phil Karlton, Netscape"]
""
There are only two hard things in Computer Science: cache invalidation and naming things.
""

== Was ist Caching?

[%step]
[.preload]
* Ich halte eine lokale Kopie von Daten _f√ºr eine begrenzte Zeit_
// Daten sind aufwendig zu beschaffen und √§ndern sich eher selten.
+
-> Ich tausche Speicher gegen $Vorteile
* $Vorteile beinhalten:
[%step]
** Bessere Performance: Langsame Festplatten-, Netzwerk-, Datenbank-Zugriffe reduzieren; aufwendige (Neu-)Berechnungen gleicher Werte vermeiden
** Lastvermeidung: Nicht/schlecht skalierbares (Legacy-)Backend sch√ºtzen; Aufrufe kostenpflichtiger externer APIs minimieren
// Caches sind in der Regel leichter skalierbar als der typische Legacy-Monolith
** Fehlertoleranz: Weiterarbeiten k√∂nnen, obwohl ein externes System nicht erreichbar ist
// -> Resilience

== Rahmenbedingungen

[%step]
[.preload]
* M√∂glichst hohe Cache Hit Rate
+
-> Anzahl der Anfragen, die aus dem Cache beantwortet werden k√∂nnen
* M√∂glichst niedriger Speicherverbrauch
+
-> Begrenzt durch Kapazit√§t und/oder Kosten
// L√∂sung: Neue Eintr√§ge ersetzen bestehende (FIFO - First In First Out, LRU - Least Recently Used, LFU - Least Frequently Used)
// L1-Cache in der CPU ist AFAIK immer noch < 1 MB
* M√∂glichst keine √úberalterung
+
-> Aktualisierungen vom Quellsystem mitbekommen
// L√∂sung: Eintr√§ge werden nur f√ºr bestimmte G√ºltigkeitsdauer gespeichert (TTL - Time to Live)

[role="nofooter"]
== Cache Stampede

[caption="https://www.flickr.com/photos/dennissylvesterhurd/8981216419/"]
image::8981216419_1f732d4534_k.jpg[background, size=cover]

== Cache Stampede

[,Wikipedia]
""
A cache stampede is a type of cascading failure that can occur when massively parallel computing systems with caching mechanisms come under very high load. This behaviour is sometimes also called dog-piling.
""
// Aka: "Thundering Herd"

== Eine typische Implementierung

[source,java,indent=0]
----
include::{src}/Caching.java[tag=fetch]
----

[%step]
* Eintr√§ge werden aus dem Cache geladen
* Falls Eintrag nicht vorhanden ist (z.B. wegen √úberalterung), wird er neu berechnet und im Cache abgelegt
* Ideale Welt: Auf Eintr√§ge im Cache wird gleichverteilt zugegriffen
* Cache Hit Rate von 80%, 100 Requests pro Sekunde
+
-> 80 aus dem Cache bedient, 20 neu berechnet
// Keine Synchonisation

== Es war einmal...

[%step]
* Situation: Cache mit einem einzigen Eintrag
* ...der von einem externen System kommt
* ...f√ºr jeden Request gebraucht wird
* ...und eine TTL von 4 Stunden hat
* Cache Hit Rate von 100%, 20 Requests pro Sekunde
+
-> 20 aus dem Cache bedient, 0 an das externe System

[.fragment]
üòé

== ...ein b√∂ses Erwachen

image::requests-white-3.png[]

[.fragment]
üò¢

== TTL + 1 Sekunde

[%step]
* 20 Requests pro Sekunde
* ...stellen fest, dass der Eintrag veraltet ist
// Und weil v√∂llig isoliert und unabh√§ngig voneinander:
* ...laden den Eintrag neu vom externen System
+
-> 20 Requests pro Sekunde an das externe System
* Ein Request dauert 6 Sekunden
* 20 Requests pro Sekunde stellen 6 Sekunden lang fest, dass der Eintrag veraltet ist
+
-> 120 Requests an das externe System
* Hei√üt auch: Unter Umst√§nden 120 blockierte Ressourcen (Verbindungen, Threads, ...) und ein √ºberlastetes externes System, l√§ngere Antwortzeiten, ...
// Antworten dauern l√§nger, triggern Retries, √ºberlastete Netzwerke, √ºberlastete Datenbanken, ...
// -> Congestion Collapse

[role="nofooter"]
== Congestion Collapse

[caption="https://www.flickr.com/photos/x1brett/2212516746/"]

image::2212516746_11142f0ab6_o.jpg[background, size=cover]

== Was kann man tun?

* Locking
* Externe Aktualisierung
* Probabilistic Early Expiration

== Locking

[%step]
[.preload]
* Der erste Zugriff sperrt den Eintrag und berechnet ihn neu
* Alle anderen warten auf die Aktualisierung
* Nur ein Zugriff an das externe System

=== Implementierung

[source,java,indent=0]
----
include::{src}/Caching.java[tag=locking]
----

[.attribution]
Avoiding cache stampede at DoorDash:
https://medium.com/@DoorDash/avoiding-cache-stampede-at-doordash-55bbf596d94b

=== Einschr√§nkungen

[%step]
* Synchronisierung ist komplex
+
-> Deadlocks, Starvation, ...
* Zus√§tzliche Schreib- und Lesezugriffe f√ºr den Lock
* Lock selbst braucht "passende" TTL
+
-> Weder zu lang noch zu kurz
* "Alle anderen warten auf die Aktualisierung"
* Plus: Locking im verteilten System?
+
-> Netzwerk, Latenz, ...
+
-> Was, wenn genau der Knoten wegbricht, der gerade den Lock hat?

== Externe Aktualisierung

[%step]
[.preload]
* Separater Prozess, der Eintr√§ge neu berechnet
+
-> Daemon Thread, Cronjob, manueller Trigger, ...
* Einzelne Requests laden nicht mehr nach
* Nur ein Zugriff an das externe System
// Vielleicht mit Batch-Fetch sogar noch weniger als mit Locking
* Erm√∂glicht zeitgesteuerte Aktualisierung unabh√§ngig von Requests (st√ºndliche Reports, ...)

=== Implementierung

[source,java,indent=0]
----
include::{src}/Caching.java[tag=external]
----

=== Einschr√§nkungen

[%step]
* Eine Komponente mehr, die betrieben werden will
+
-> Fallback, wenn der separate Prozess nicht funktioniert
* Funktioniert nicht "on the fly"
+
-> Eintr√§ge basierend auf beliebigen Nutzereingaben?
* Typischerweise deutlich h√∂herer Speicherbedarf und unn√∂tige Neuberechnungen
// Weil alle Eintr√§ge im Cache vorgehalten werden m√ºssen und der separate Prozess nicht wei√ü, was tats√§chlich zugegriffen wird
+
-> Nach welchen Kriterien aktualisieren?
+
-> 1 Million Artikel, von denen 80% (fast) nie verkauft werden?

// Beides hat sicher Anwendungsf√§lle:
// Aktuelle Temperatur immer zur vollen Stunde; alle Preise, weil 100% Resilience

== Probabilistic Early Expiration

[%step]
[.preload]
* Eintr√§ge aktualisieren, _solange sie noch g√ºltig sind_
* Der alte Eintrag bleibt im Cache und kann weiter verwendet werden
* Prinzip: "Verhalte dich so als w√§re es Jetzt + X"
+
-> X wird durch Wahrscheinlichkeitsfunktion zuf√§llig bestimmt
* Jeder Request hat die Chance, den Eintrag neu zu berechnen; die Wahrscheinlichkeit daf√ºr steigt in Richtung TTL
* Dank Wahrscheinlichkeitsfunktion auch verteilt ohne Synchronisation und Locking nutzbar
* Verschiedene Wahrscheinlichkeitsfunktionen m√∂glich
// Wahrscheinlichkeitsfunktion ist der wichtige Teil

=== Paper

[%step]
[.preload]
* "Optimal Probabilistic Cache Stampede Prevention"
+
Andrea Vattani, Flavio Chierichetti, Keegan Lowenstein
+
https://cseweb.ucsd.edu/~avattani/papers/cache_stampede.pdf (CC BY-NC-ND 3.0)
// University of California San Diego
// CC BY-NC-ND 3.0 (https://creativecommons.org/licenses/by-nc-nd/3.0/)
* Hier: Wahrscheinlichkeit steigt exponentiell in der N√§he der TTL, TTL wird m√∂glichst gut eingehalten
* Behauptung: Optimal f√ºr alle Anwendungsf√§lle

[%notitle]
=== Paper

[.split-left]
image::paper1.png[]

[.split-right]
image::paper2.png[]

=== Die Wahrscheinlichkeitsfunktion

--
delta * beta * ln(random())
--

[%step]
* random: Zufallszahl zwischen 0 und 1
* ln: nat√ºrlicher Logarithmus
* delta: Zeitbedarf f√ºr Neuberechnung
* beta: "Einstellknopf" - empfohlener Standard 1.0
** > 1.0 bewirkt eher fr√ºhere Neuberechnung
** < 1.0 bewirkt eher sp√§tere (also n√§her an der tats√§chlichen TTL)
* Unabh√§ngig von konkreter TTL

[%notitle]
=== Auswirkungen

[.split-left]
image::b1d10.png[]
image::b4d10.png[]

[.split-right]
image::b1d40.png[]
image::b025d10.png[]

=== Implementierung

[source,java,indent=0]
----
include::{src}/Caching.java[tag=xfetch]
----

=== Implementierungsdetail

[.split-left]
[%step]
[.preload]
* Math.random() liefert [0, 1)
* ln(0) ist mathematisch nicht definiert, geht gegen -Unendlich
* Java kann das ab:
+
[source,java]
----
jshell> (long)java.lang.Math.log(0)
$1 ==> -9223372036854775808
----
// https://tryjshell.org/

[.split-right]
image::ln.png[size=25%, float=right]
// https://www.desmos.com/calculator

=== Einschr√§nkungen

[%step]
* Mehr Speicherverbrauch
// Wir speichern neben dem Eintrag auch noch die Berechnungsdauer und die TTL
// (Falls das der Cache nicht sowieso schon f√ºr uns tut)
* Wahrscheinlichkeitsfunktion bietet keine Garantien
[%step]
** Trotzdem noch mehr als 1 Request gleichzeitig m√∂glich (aber Gr√∂√üe der Stampede deutlich geringer)
** Theoretisch auch direkt erneute Neuberechnung m√∂glich (aber sehr unwahrscheinlich)
* Hilft nicht bei leerem Cache
// Weil: Kein alter Eintrag, und wenn TTL erreicht ist, schlagen doch wieder 100% zu
//-> Cache "vorw√§rmen"
//-> Kombination mit Locking

// == Further Reading

// Doordash:
// https://medium.com/@DoorDash/avoiding-cache-stampede-at-doordash-55bbf596d94b

// YouTube:
// "A right and creative solution are to introduce a jitter ‚Äì randomizing (a little) the expiration time for each cache. This solution was introduced by Youtube (This video does not talk only about the Thundering herd problem, and it is a little bit outdated, but it is still relevant)."
// Jitter - "zuf√§llige TTL zwischen 18 und 30 Stunden statt genau 24 Stunden"
// https://youtu.be/G-lGCC4KKok?t=1093

// Facebook:
// "Another excellent resource to learning about how to solve The Thundering Problem in a much more complex scenario, I recommend this short video, from the Facebook Engineering team."
// https://www.facebook.com/Engineering/videos/10153675295382200/?v=10153675295382200

// RedisConf 2017:
// https://www.youtube.com/watch?v=1sKn4gWesTw